{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "28MNIST_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJZ7M2F_vutD"
      },
      "source": [
        "!git clone https://github.com/ninomiyalab/Memory_Less_Momentum_Quasi_Newton"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYQBxNVov4xM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Conv2D, Flatten\n",
        "from tensorflow.keras import optimizers\n",
        "from Memory_Less_Momentum_Quasi_Newton.MLQN import *\n",
        "from Memory_Less_Momentum_Quasi_Newton.MLMoQ import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def compare_MNIST(i = 0):\n",
        "    np.random.seed(i)\n",
        "    \n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "  \n",
        "    x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
        "    y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "    #defined Neural Network Model\n",
        "    def My_Model(input_shape, Output_dim):\n",
        "        inputs = Input(shape = (input_shape))\n",
        "        x = Flatten()(inputs)\n",
        "        x = Dense(10, activation=\"sigmoid\")(x)\n",
        "        outputs = Dense(Output_dim, activation=\"softmax\")(x)\n",
        "        model = Model(inputs = [inputs], outputs = [outputs])\n",
        "        return model\n",
        "    \n",
        "    model = My_Model(x_train.shape[1:], Output_dim=10)\n",
        "    \n",
        "    model.save(\"model.h5\")\n",
        "    \n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    epochs = 2000\n",
        "\n",
        "    verbose = False\n",
        "\n",
        "    graph = True\n",
        "\n",
        "    # MLQN Training \n",
        "    # --------------------------------------------------------------------------------------\n",
        "    model = load_model(\"model.h5\")\n",
        "\n",
        "    optimizer = MLQN( )\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    MLQN_history = model.fit(x_train, y_train, epochs = epochs, verbose = verbose, batch_size = x_train.shape[0], validation_data = (x_test, y_test))\n",
        "    # --------------------------------------------------------------------------------------\n",
        "\n",
        "    # MLMoQ Training\n",
        "    # --------------------------------------------------------------------------------------\n",
        "    model = load_model(\"model.h5\")\n",
        "\n",
        "    optimizer = MLMoQ()\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    MLMoQ_history = model.fit(x_train, y_train, epochs = epochs, verbose = verbose, batch_size = x_train.shape[0], validation_data = (x_test, y_test))\n",
        "    # --------------------------------------------------------------------------------------\n",
        "\n",
        "    # Adam Training \n",
        "    # --------------------------------------------------------------------------------------\n",
        "    model = load_model(\"model.h5\")\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    \n",
        "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    Adam_history = model.fit(x_train, y_train, epochs = epochs, verbose = verbose, batch_size = x_train.shape[0], validation_data = (x_test, y_test))\n",
        "    # --------------------------------------------------------------------------------------\n",
        "\n",
        "    if graph:\n",
        "        fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
        "        #Train Loss vs. Iteration graph\n",
        "        axL.set_title(\"Train_Loss\")\n",
        "        axL.plot(MLQN_history.history['loss'],color=\"blue\", label=\"MLQN\")\n",
        "        axL.plot(MLMoQ_history.history['loss'], color=\"m\",label=\"MLMoQ\")\n",
        "        axL.plot(Adam_history.history['loss'], color=\"orange\",label=\"Adam\")\n",
        "        axL.set_xlabel('Iterations')\n",
        "        axL.set_ylabel('Train Loss')\n",
        "        axL.legend(bbox_to_anchor=(0, -0.2), loc='upper left', borderaxespad=0)\n",
        "        axL.legend()\n",
        "        #Train Accuracy vs. Iteration graph\n",
        "        axR.set_title(\"Train_Accuracy\")\n",
        "        axR.plot(MLQN_history.history['accuracy'],color=\"blue\", label=\"MLQN\")\n",
        "        axR.plot(MLMoQ_history.history['accuracy'], color=\"m\",label=\"MLMoQ\")\n",
        "        axR.plot(Adam_history.history['accuracy'],color=\"orange\", label=\"Adam\")\n",
        "        axR.set_xlabel('Iterations')\n",
        "        axR.set_ylabel('Train Accuracy')\n",
        "        axR.legend(bbox_to_anchor=(0, -0.2), loc='upper left', borderaxespad=0)\n",
        "        axR.legend()\n",
        "        plt.show()\n",
        "        \n",
        "        fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
        "        #Test Loss vs. Iteration graph\n",
        "        axL.set_title(\"Test_Loss\")\n",
        "        axL.plot(MLQN_history.history['val_loss'],color=\"blue\", label=\"MLQN\")\n",
        "        axL.plot(MLMoQ_history.history['val_loss'],color=\"m\", label=\"MLMoQ\")\n",
        "        axL.plot(Adam_history.history['val_loss'],color=\"orange\",label=\"Adam\")\n",
        "        axL.set_xlabel('Iterations')\n",
        "        axL.set_ylabel('Test Loss')\n",
        "        axL.legend(bbox_to_anchor=(0, -0.2), loc='upper left', borderaxespad=0)\n",
        "        axL.legend()\n",
        "\n",
        "        #Test Accuracy vs. Iteration graph\n",
        "        axR.set_title(\"Test_Accuracy\")\n",
        "        axR.plot(MLQN_history.history['val_accuracy'],color=\"blue\",label=\"MLQN\")\n",
        "        axR.plot(MLMoQ_history.history['val_accuracy'],color=\"m\", label=\"MLMoQ\")\n",
        "        axR.plot(Adam_history.history['val_accuracy'],color=\"orange\", label=\"Adam\")\n",
        "        axR.set_xlabel('Iterations')\n",
        "        axR.set_ylabel('Test Accuracy')\n",
        "        axR.legend(bbox_to_anchor=(0, -0.2), loc='upper left', borderaxespad=0)\n",
        "        axR.legend()\n",
        "        plt.show()\n",
        "        \n",
        "        \n",
        "\n",
        "for i in range(10):\n",
        "    print(i + 1)\n",
        "    compare_MNIST(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}